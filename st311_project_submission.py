# -*- coding: utf-8 -*-
"""st311-project-submission.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FfmfsXw8wgEdWhYHRefH5qnuRe8SQBGy

#Differing Machine Learning Architectures for Histopathological Cancer Detection

Candidate numbers: 28551, 33349

## Mounting the Google drive for the JSON API Key
"""

# Retrieving the JSON API Key from the drive to grant access to the Kaggle dataset
from google.colab import drive
drive.mount('/content/drive')

"""## Retrieving the dataset from Kaggle, validating API and unzipping"""

# Downloading the Kaggle dataset and unzipping it
! pip install kaggle
! mkdir ~/.kaggle
! cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json
! chmod 600 ~/.kaggle/kaggle.json
! kaggle competitions download histopathologic-cancer-detection
! unzip histopathologic-cancer-detection

"""## Loading the required dependencies"""

# Commented out IPython magic to ensure Python compatibility.
import os #For training time
import numpy as np #For calculations
import pandas as pd #For manipulating the data in tabular form
from PIL import Image #For plotting images
import matplotlib.pyplot as plt #For plotting images from the data
import matplotlib.pyplot as plt
# %matplotlib inline
from PIL import Image #For displaying images
from skimage.io import imread #For reading image data
from sklearn.model_selection import train_test_split #For splitting data into test and train sets
import torch #For pytorch
import torchvision #For image manipulation 
from torchvision import models, transforms #For image transformation and pre-trained models
import torch.nn as nn #PyTorch module for creating and training neural networks
import torch.nn.functional as F #For implementing activation functions
from torch.utils.data import TensorDataset, DataLoader, Dataset #For the dataloader
import torch.optim as optim #For the optimiser in the training function
import cv2 #For displaying images
from tqdm import tqdm #For the progress meter during training
!pip install tensorboard #For visualising model performance
from torch.utils.tensorboard import SummaryWriter #For model summary

"""## Loading and subsetting data

### Setting the dataloader path and downloading the lavels
"""

train_path = "train" #Training path
test_path = "test" #Test path
df_train = pd.read_csv("train_labels.csv") #Loading the training data

"""### Splitting the data into train and validation sets"""

#Splitting the training data into training and validation sets

train_df, validate_df = train_test_split(df_train, test_size=0.2)
 
# Printing the sizes of the training and validation sets after the split

print(f'{len(os.listdir(train_path))} pictures in the training set.')
print(f'{len(os.listdir(test_path))} pictures in validation set.')

#Plotting the proportion of labels in the training set

plt.pie(train_df.label.value_counts(), labels=['No Cancer', 'Cancer'], 
        colors=['#0099FF', '#CC3399'], autopct='%1.1f', startangle=90)
plt.style.use('classic')
plt.show()

# Plotting a grid of random images chosen from the test set
fig = plt.figure(figsize=(15, 5))
train_imgs = os.listdir("train/")
for i, img in enumerate(np.random.choice(train_imgs, 10)):
    ax = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])
    im = Image.open("train/" + img)
    plt.imshow(im)
    tlt = df_train.loc[df_train['id'] == img.split('.')[0], 'label'].values[0]
    ax.set_title(f'Label: {tlt}')

"""### Creating the custom dataset class"""

class CancerDataset(Dataset):
    '''
    This custom class allows us to perform transformations on the images in the dataset
    '''
    def __init__(self, df_data, data_dir = './', transform=None):
        super().__init__()
        self.df = df_data.values
        self.data_dir = data_dir
        self.transform = transform
        
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, index):
        img_name,label = self.df[index]
        img_path = os.path.join(self.data_dir, img_name + '.tif')
        image = cv2.imread(img_path)

        if self.transform is not None:
            image = self.transform(image)

        return image, label

"""### Defining the image resizing function"""

'''
This function allows us to compose several transformations on the images in the training and validation sets such as flips and rotations, as well as to normalize the matrix of these images.
This function also allows us to resize the images to a specified size. 
'''

def resizeImages(size):
    transform_train = transforms.Compose([transforms.ToPILImage(),
                                      transforms.RandomHorizontalFlip(), 
                                      transforms.RandomVerticalFlip(),
                                      transforms.RandomRotation(20), 
                                      transforms.ToTensor(),
                                      transforms.Resize(size),
                                      transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])
    
    transform_val = transforms.Compose([transforms.ToPILImage(),
                                      transforms.ToTensor(), 
                                      transforms.Resize(size),
                                      transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])
    
    transform_test = transforms.Compose([transforms.ToPILImage(), 
                                      transforms.ToTensor(),
                                      transforms.Resize(size),
                                      transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])
    batch_size = 100
    train_dataset = CancerDataset(df_data=train_df[1:10000], data_dir=train_path, transform=transform_train)
    val_dataset = CancerDataset(df_data=validate_df[1:5000], data_dir=train_path, transform=transform_val)
    
    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)
    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)
    
    return train_dataloader, val_dataloader

"""### Defining the training function"""

'''
Defining the helper function to get the number of correct predictions
'''
def get_num_correct(preds, labels):
    return preds.argmax(dim=1).eq(labels).sum().item()

'''
Defining the trainer function that learns a neural network given as input.
'''
def modelTrain(net, train_dl, val_dl, criterion, optimizer, epoch_num, learning_rate, scheduler):
  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #Training with either CPU or GPU
  print(device)
  model = net.to(device=device) #Copies the model to the device
  loss_hist_train=[] #Storing the training loss
  loss_hist_val=[] #Storing the validation loss

  tb = SummaryWriter() #Initialising the tensorBoard summary writer
  for epoch in tqdm(range(epoch_num)): #Iterating over the epochs
      train_loss = 0.0 #Initialising the training loss to 0
      valid_loss = 0.0 #Initialising the validation loss to 0
      total_correct=0 #Initialising the number of correct predictions to 0
      model.train() #Setting the model to training mode
    
      for batch_idx, (data, targets) in enumerate(train_dl): #Iterating over the training data loader
          data = data.to(device=device) #Copies the data to the device
          targets = targets.to(device=device) #Copies the target variables to the device

          ## Forward Pass
          output = model(data) #Computing the output values
          loss = criterion(output,targets) #Computing the loss
          optimizer.zero_grad() #Setting the gradient of the loss in the optimizer to 0 for all parameters
          loss.backward() #Computing the gradient of the loss with respect to all parameters
          optimizer.step() #Optimizer updates the value of all parameters
          train_loss += loss.item() #Recording the loss
          loss_hist_train.append(train_loss) #Appending the current loss to the history list
          total_correct+= get_num_correct(output, targets) #Getting the total number of correct predictions

      #Visualising the loss and accuracy in tensorBoard
      tb.add_scalar("Loss", train_loss, epoch)
      tb.add_scalar("Correct", total_correct, epoch)
      tb.add_scalar("Train Accuracy", total_correct/ 10000, epoch)


      print(f"Loss in epoch {epoch} | {train_loss/len(train_dl)}")
      #Setting the model to evaluation mode
      model.eval()
      with torch.no_grad():
          num_correct = 0
          num_samples = 0
          for batch_idx, (data,targets) in enumerate(val_dl):
              data = data.to(device=device)
              targets = targets.to(device=device)
              output = model(data)
              _, predictions = torch.max(output.data, 1) #Getting the predictions in the format of labels
              num_correct += (predictions == targets).sum() #Getting the number of correct predictions
              num_samples += predictions.size(0) #Getting the total number of predictions
              tb.add_scalar("Validation Accuracy", num_correct/ num_samples, epoch)
          print(f"Got {num_correct} / {num_samples} with accuracy {float(num_correct) / float(num_samples) * 100:.2f}")
        
      #For the learning rate decay
      scheduler.step()
  tb.close()
  return model

"""## A Convolutional Neural Network Architecture (Baseline model)"""

'''
Defining a CNN architecture with 5 blocks.
Each block consists of a convolutional layer, a batch normalisation layer, a ReLU activation function and a max-pooling layer.
'''
class CNN(nn.Module):
    def __init__(self):
        super(CNN,self).__init__()
        
        self.conv1 = nn.Sequential(
                        nn.Conv2d(3, 32, 3, stride=1, padding=1),
                        nn.BatchNorm2d(32),
                        nn.ReLU(inplace=True),
                        nn.MaxPool2d(2,2))
        
        self.conv2 = nn.Sequential(
                        nn.Conv2d(32, 64, 3, stride=1, padding=1),
                        nn.BatchNorm2d(64),
                        nn.ReLU(inplace=True),
                        nn.MaxPool2d(2,2))
        
        self.conv3 = nn.Sequential(
                        nn.Conv2d(64, 128, 3, stride=1, padding=1),
                        nn.BatchNorm2d(128),
                        nn.ReLU(inplace=True),
                        nn.MaxPool2d(2,2))
        
        self.conv4 = nn.Sequential(
                        nn.Conv2d(128, 256, 3, stride=1, padding=1),
                        nn.BatchNorm2d(256),
                        nn.ReLU(inplace=True),
                        nn.MaxPool2d(2,2))
        
        self.conv5 = nn.Sequential(
                        nn.Conv2d(256, 512, 3, stride=1, padding=1),
                        nn.BatchNorm2d(512),
                        nn.ReLU(inplace=True),
                        nn.MaxPool2d(2,2))
        
        # Defining the fully connected MLP

        self.fc=nn.Sequential(
                nn.Linear(512, 256),
                nn.ReLU(inplace=True),
                nn.BatchNorm1d(256),
                nn.Dropout(0.4),
                nn.Linear(256, 2),
                nn.Sigmoid()) #Including a sigmoid function for numerical stability
        
    # Defining the forward pass

    def forward(self,x):
        x=self.conv1(x)
        x=self.conv2(x)
        x=self.conv3(x)
        x=self.conv4(x)
        x=self.conv5(x)
        #print(x.shape)
        x=x.view(x.shape[0],-1)
        x=self.fc(x)
        return x

"""#### Training the basic CNN model"""

# Commented out IPython magic to ensure Python compatibility.
train_dl, val_dl = resizeImages(32) #Resizing the images to size 32x32
print('Size of the training data loader: {}'.format(len(train_dl)),'Size of the test data loader: {}'.format(len(val_dl)))

tb = SummaryWriter() #Initialising tensorBoard
model = CNN() #Initialising the model

#Plotting images from the training dataloader to tensorBoard
images, labels = next(iter(train_dl))
grid = torchvision.utils.make_grid(images)
tb.add_image("images", grid)
tb.add_graph(model, images)
tb.close()

learning_rate = 0.02 #Setting the initial learning rate
criterion = nn.CrossEntropyLoss() #Defining the loss function (Cross Entropy)
optimizer = optim.Adam(model.parameters(), lr= learning_rate) #Initialising the Adam Optimiser
decayRate = 0.96 #Setting the decay rate
lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate) #Defining an exponential decay
modelTrain(model, train_dl, val_dl, criterion, optimizer, 10, learning_rate, lr_scheduler) #Training the network

# %load_ext tensorboard
# %tensorboard --logdir runs

"""## The AlexNet Architecture

"""

'''
Defining the AlexNet architecture consiting of 8 weight layers from which 5 are convolutional and 3 are fully connected.
'''

class AlexNet(nn.Module):
    def __init__(self):
        super(AlexNet, self).__init__()
        #Convolutional layers
        self.conv1 = nn.Conv2d(in_channels=3, out_channels= 96, kernel_size= 11, stride=4, padding=0 )
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)
        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride= 1, padding= 2)
        self.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride= 1, padding= 1)
        self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1)
        self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1)
        #Fully connected layers
        self.fc1  = nn.Linear(in_features= 9216, out_features= 4096)
        self.fc2  = nn.Linear(in_features= 4096, out_features= 4096)
        self.fc3 = nn.Linear(in_features=4096 , out_features=2)
        

    #Defining the forward pass

    def forward(self,x):
        x = F.relu(self.conv1(x))
        x = self.maxpool(x)
        x = F.relu(self.conv2(x))
        x = self.maxpool(x)
        x = F.relu(self.conv3(x))
        x = F.relu(self.conv4(x))
        x = F.relu(self.conv5(x))
        x = self.maxpool(x)
        x = x.reshape(x.shape[0], -1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

"""#### Training the AlexNet model"""

# Commented out IPython magic to ensure Python compatibility.
train_dl, val_dl = resizeImages(227) #Resizing the images to 227x227
print('Size of the training data loader: {}'.format(len(train_dl)),'Size of the test data loader: {}'.format(len(val_dl)))

tb = SummaryWriter() #Initialising tensorBoard
model = AlexNet() #Initialising the model

#Plotting images from the training dataloader to tensorBoard
images, labels = next(iter(train_dl))
grid = torchvision.utils.make_grid(images)
tb.add_image("images", grid)
tb.add_graph(model, images)
tb.close()

learning_rate = 0.001 #Initialising the learning rate
criterion = nn.CrossEntropyLoss() #Defining the loss function (Cross Entropy)
optimizer = optim.Adam(model.parameters(), lr= learning_rate) #Initialising the Adam optimiser
decayRate = 0.96 #Setting the decay rate
lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate) #Defining an exponential decay
modelTrain(model, train_dl, val_dl, criterion, optimizer, 10, learning_rate, lr_scheduler) #Training the network

# %tensorboard --logdir runs

"""## The ResNet18 Architecture (Pre-trained)"""

# Importing the pre-trained model from torchvision models

resnet = models.resnet18(pretrained=True)
resnet

# Adjusting the final fully-connected layer to suit the binary classification problem

resnet.fc = torch.nn.Sequential(
    torch.nn.Linear(
        in_features=512,
        out_features=2 #Modying the number of output features to 2
    ),
    torch.nn.Sigmoid()
    )

"""####Training the ResNet18 model"""

# Commented out IPython magic to ensure Python compatibility.
train_dl, val_dl = resizeImages(224) #Resizing the images to size 224x224
print('Size of the training data loader: {}'.format(len(train_dl)),'Size of the test data loader: {}'.format(len(val_dl)))

tb = SummaryWriter() #Initialising tensorBoard

#Plotting images from the training dataloader to tensorBoard
images, labels = next(iter(train_dl))
grid = torchvision.utils.make_grid(images)
tb.add_image("images", grid)
tb.add_graph(resnet, images)
tb.close()

learning_rate = 0.001 #Initialising the learning rate
criterion = nn.CrossEntropyLoss() #Defining the loss function (Cross Entropy)
optimizer = optim.Adam(model.parameters(), lr= learning_rate) #Initialising the Adam optimiser
decayRate = 0.96 #Setting the decay rate
lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate) #Defining an exponential decay
modelTrain(resnet, train_dl, val_dl, criterion, optimizer, 10, learning_rate, lr_scheduler) #Training the model

# %tensorboard --logdir runs

"""## The VGG11 Architecture"""

'''
Defining the VGG11 architecture consiting of 11 weight layers from which 8 are convolutional and 3 are fully connected.
'''
class VGG11(nn.Module):
    def __init__(self):
        super(VGG11, self).__init__()
        self.conv_layers = nn.Sequential(
            #Convolutional layers
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        #Fully connected layers
        self.linear_layers = nn.Sequential(
            nn.Linear(in_features=512*7*7, out_features=4096),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(in_features=4096, out_features=4096),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(in_features=4096, out_features=2)
        )

    #Defining the forward pass
    def forward(self, x):
        x = self.conv_layers(x)
        #Flattening to prepare for the fully connected layers
        x = x.view(x.size(0), -1)
        x = self.linear_layers(x)
        return x

"""####Training the VGG11 model"""

# Commented out IPython magic to ensure Python compatibility.
train_dl, val_dl = resizeImages(224) #Resizing the images to size 224x224
print('Size of the training data loader: {}'.format(len(train_dl)),'Size of the test data loader: {}'.format(len(val_dl)))

model = VGG11() #Initialising the model
tb = SummaryWriter() #Initialising tensorBoard

learning_rate = 0.001 #Initialising the learning rate
criterion = nn.CrossEntropyLoss() #Defining the loss function (Cross Entropy)
optimizer = optim.Adam(model.parameters(), lr= learning_rate) #Initialising the Adam optimiser
decayRate = 0.96 #Setting the decay rate 
lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate) #Defining an exponential decay 
modelTrain(model, train_dl, val_dl, criterion, optimizer, 10, learning_rate, lr_scheduler) #Training the model
# %tensorboard --logdir runs